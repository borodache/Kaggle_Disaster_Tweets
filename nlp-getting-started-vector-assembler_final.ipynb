{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-09T15:40:49.861670Z","iopub.execute_input":"2023-04-09T15:40:49.862107Z","iopub.status.idle":"2023-04-09T15:40:49.871090Z","shell.execute_reply.started":"2023-04-09T15:40:49.862072Z","shell.execute_reply":"2023-04-09T15:40:49.869733Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install sparknlp","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:40:49.877883Z","iopub.execute_input":"2023-04-09T15:40:49.878252Z","iopub.status.idle":"2023-04-09T15:41:01.617208Z","shell.execute_reply.started":"2023-04-09T15:40:49.878219Z","shell.execute_reply":"2023-04-09T15:41:01.615599Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Requirement already satisfied: sparknlp in /opt/conda/lib/python3.7/site-packages (1.0.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sparknlp) (1.21.6)\nRequirement already satisfied: spark-nlp in /opt/conda/lib/python3.7/site-packages (from sparknlp) (4.3.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:01.619514Z","iopub.execute_input":"2023-04-09T15:41:01.619890Z","iopub.status.idle":"2023-04-09T15:41:13.616641Z","shell.execute_reply.started":"2023-04-09T15:41:01.619852Z","shell.execute_reply":"2023-04-09T15:41:13.615453Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyspark in /opt/conda/lib/python3.7/site-packages (3.3.2)\nRequirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.7/site-packages (from pyspark) (0.10.9.5)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import sparknlp\nspark = sparknlp.start() \n# sparknlp.start(gpu=True) >> for training on GPU\nfrom sparknlp.base import *\nfrom sparknlp.annotator import *\nfrom pyspark.ml import Pipeline\nimport pandas as pd\nprint(\"Spark NLP version\", sparknlp.version())\nprint(\"Apache Spark version:\", spark.version)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.618586Z","iopub.execute_input":"2023-04-09T15:41:13.619005Z","iopub.status.idle":"2023-04-09T15:41:13.681852Z","shell.execute_reply.started":"2023-04-09T15:41:13.618963Z","shell.execute_reply":"2023-04-09T15:41:13.677278Z"},"trusted":true},"execution_count":19,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/4030733544.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# sparknlp.start(gpu=True) >> for training on GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sparknlp/__init__.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(gpu, apple_silicon, aarch64, memory, cache_folder, log_folder, cluster_tmp_dir, params, real_time_output, output_level)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mSparkRealTimeOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mspark_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_without_realtime_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mspark_session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sparknlp/__init__.py\u001b[0m in \u001b[0;36mstart_without_realtime_output\u001b[0;34m()\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_with_realtime_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                     \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0mjsc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0mprofiler_cls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                 \u001b[0mudf_profiler_cls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m             )\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \"\"\"\n\u001b[1;32m    401\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1586\u001b[0;31m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[1;32m   1587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2671)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2668)\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2758)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:97)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"],"ename":"Py4JJavaError","evalue":"An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2671)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2668)\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2758)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:97)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n","output_type":"error"}]},{"cell_type":"code","source":"trainDataset = spark.read \\\n  .option(\"header\", True) \\\n  .csv(\"/kaggle/input/nlp-getting-started/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.683088Z","iopub.status.idle":"2023-04-09T15:41:13.683649Z","shell.execute_reply.started":"2023-04-09T15:41:13.683429Z","shell.execute_reply":"2023-04-09T15:41:13.683456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainDataset = trainDataset.filter((trainDataset.target == 0) | (trainDataset.target == 1))\ntrainDataset","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.685103Z","iopub.status.idle":"2023-04-09T15:41:13.685908Z","shell.execute_reply.started":"2023-04-09T15:41:13.685674Z","shell.execute_reply":"2023-04-09T15:41:13.685716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql.types import IntegerType\n\ntrainDataset = trainDataset.withColumn(\"target\", trainDataset[\"target\"].cast(IntegerType()))\ntrainDataset","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.687242Z","iopub.status.idle":"2023-04-09T15:41:13.687645Z","shell.execute_reply.started":"2023-04-09T15:41:13.687445Z","shell.execute_reply":"2023-04-09T15:41:13.687465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"None_ = trainDataset.filter(trainDataset.text == None)\nNone_.show()\n# trainDataset.where(col(\"text\").isNull())","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.690006Z","iopub.status.idle":"2023-04-09T15:41:13.690862Z","shell.execute_reply.started":"2023-04-09T15:41:13.690511Z","shell.execute_reply":"2023-04-09T15:41:13.690548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.692920Z","iopub.status.idle":"2023-04-09T15:41:13.694287Z","shell.execute_reply.started":"2023-04-09T15:41:13.693940Z","shell.execute_reply":"2023-04-09T15:41:13.693979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Only One Hot Encoding","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import udf, col\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml.linalg import DenseVector, VectorUDT\nfrom sparknlp.base import DocumentAssembler\nfrom sparknlp.annotator import UniversalSentenceEncoder\nfrom pyspark.ml.linalg import DenseVector, VectorUDT\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.functions import udf, explode\nfrom pyspark.ml.feature import SQLTransformer\n\n# Define the pipeline stages\nstages_one_hot = []\n\n# Define the categorical columns\ncategorical_cols = [\"keyword\", \"location\"]\n\nfor col in categorical_cols:\n    indexer = StringIndexer(inputCol=col, outputCol=col+\"_index\")\n    indexer.setHandleInvalid(\"keep\")\n    encoder = OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_vec\")\n    stages_one_hot += [indexer, encoder]\n\nstages_together = []\nassembler = VectorAssembler(inputCols=[col+\"_vec\" for col in categorical_cols],\n                            outputCol=\"features\")\nstages_together += [assembler]\n\n# Add the LogisticRegression stage with the target column as the label\n# lr = LogisticRegression(featuresCol=\"features\", labelCol=\"target\")\n# stages_together += [lr]\n\nclasssifierdl = ClassifierDLApproach()\\\n  .setInputCols([\"features\"])\\\n  .setOutputCol(\"class\")\\\n  .setLabelColumn(\"target\")\\\n  .setMaxEpochs(5)\\\n  .setEnableOutputLogs(True)\nstages_together += [classsifierdl]\n\n# Create the pipeline\nstages=stages_one_hot + stages_together\nfor stage in stages:\n    print(f\"{str(stage)} - {str(type(stage))}\")\n    if not (isinstance(stage, Estimator) or isinstance(stage, Transformer)):\n        print((\"Cannot recognize a pipeline stage of type %s.\" % type(stage)))\npipeline = Pipeline(stages=stages)\n\ntrainDataset = trainDataset.withColumn(\"target\", trainDataset[\"target\"].cast(IntegerType()))\ndf = trainDataset\n\n# Fit the pipeline to the data\npipelineModel = pipeline.fit(df)\n\n# # Apply the pipeline to the data and get the predictions\ntransformed_df = pipelineModel.transform(df)\ntransformed_df","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.696369Z","iopub.status.idle":"2023-04-09T15:41:13.697425Z","shell.execute_reply.started":"2023-04-09T15:41:13.697103Z","shell.execute_reply":"2023-04-09T15:41:13.697138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Everything together logistic regression","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import udf, col\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml.linalg import DenseVector, VectorUDT\nfrom sparknlp.base import DocumentAssembler\nfrom sparknlp.annotator import UniversalSentenceEncoder\nfrom pyspark.ml.linalg import DenseVector, VectorUDT\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.functions import udf, explode\nfrom pyspark.ml.feature import SQLTransformer\n\n# Define the pipeline stages\nstages_one_hot = []\n\n# Define the categorical columns\ncategorical_cols = [\"keyword\", \"location\"]\n\nfor col in categorical_cols:\n    indexer = StringIndexer(inputCol=col, outputCol=col+\"_index\")\n    indexer.setHandleInvalid(\"keep\")\n    encoder = OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_vec\")\n    stages_one_hot += [indexer, encoder]\n\nstages_text = []\n# Add the DocumentAssembler stage\ndocumentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\nstages_text += [documentAssembler]\n\n# Add the UniversalSentenceEncoder stage\nencoder = UniversalSentenceEncoder.pretrained().setInputCols([\"document\"]).setOutputCol(\"sentence_embedding\")\nstages_text += [encoder]\n\nstages_together = []\nconvert_to_vector_udf = spark.udf.register(\n    \"convert_to_vector_udf\",\n    lambda r : Vectors.dense(r[0][5]), \n    VectorUDT()\n)\n\nstages_text += [SQLTransformer(\n#     statement = \"SELECT convert_to_vector_udf(keyword_vec) keyword_vec_densed, convert_to_vector_udf(location_vec) location_vec_densed, convert_to_vector_udf(sentence_embedding) sentence_embedding_densed, target FROM __THIS__\")]\n    statement = \"SELECT *, convert_to_vector_udf(sentence_embedding) sentence_embedding_densed FROM __THIS__\")]\n\ninputCols=[col+\"_vec\" for col in categorical_cols] + [\"sentence_embedding_densed\"]\n# inputCols=[\"sentence_embedding_densed\"]\nassembler = VectorAssembler(inputCols=inputCols,\n                            outputCol=\"features\")\nstages_together += [assembler]\n\n# Add the LogisticRegression stage with the target column as the label\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"target\")\nstages_together += [lr]\n\n# classsifierdl = ClassifierDLApproach()\\\n#   .setInputCols([\"features\"])\\\n#   .setOutputCol(\"class\")\\\n#   .setLabelColumn(\"target\")\\\n#   .setMaxEpochs(5)\\\n#   .setEnableOutputLogs(True)\n# stages_together += [classsifierdl]\n\n# Create the pipeline\nstages=stages_one_hot + stages_text + stages_together\n# stages=stages_text + stages_together\nfor stage in stages:\n    print(f\"{str(stage)} - {str(type(stage))}\")\n    if not (isinstance(stage, Estimator) or isinstance(stage, Transformer)):\n        print((\"Cannot recognize a pipeline stage of type %s.\" % type(stage)))\npipeline = Pipeline(stages=stages)\n\ntrainDataset = trainDataset.withColumn(\"target\", trainDataset[\"target\"].cast(IntegerType()))\ndf = trainDataset\n\n# Fit the pipeline to the data\npipelineModel = pipeline.fit(df)\n\n# # Apply the pipeline to the data and get the predictions\ntransformed_df = pipelineModel.transform(df)\ntransformed_df","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.699392Z","iopub.status.idle":"2023-04-09T15:41:13.700579Z","shell.execute_reply.started":"2023-04-09T15:41:13.700246Z","shell.execute_reply":"2023-04-09T15:41:13.700281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testDataset = spark.read \\\n    .option(\"header\", True) \\\n    .option('multiLine', 'true') \\\n    .csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.702859Z","iopub.status.idle":"2023-04-09T15:41:13.703471Z","shell.execute_reply.started":"2023-04-09T15:41:13.703163Z","shell.execute_reply":"2023-04-09T15:41:13.703199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = pipelineModel.transform(testDataset)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.705830Z","iopub.status.idle":"2023-04-09T15:41:13.707099Z","shell.execute_reply.started":"2023-04-09T15:41:13.706838Z","shell.execute_reply":"2023-04-09T15:41:13.706866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.708268Z","iopub.status.idle":"2023-04-09T15:41:13.709379Z","shell.execute_reply.started":"2023-04-09T15:41:13.709064Z","shell.execute_reply":"2023-04-09T15:41:13.709099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from pyspark.ml.linalg import Vectors, VectorUDT\n# from pyspark.sql import functions as F\n\n# ud_f = F.udf(lambda r : Vectors.dense(r), VectorUDT())\n# df = df.withColumn('result_of_vector_dense', ud_f('sentence_embedding'))\n# df","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.711029Z","iopub.status.idle":"2023-04-09T15:41:13.711744Z","shell.execute_reply.started":"2023-04-09T15:41:13.711406Z","shell.execute_reply":"2023-04-09T15:41:13.711439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pyspark.sql.functions as f\n# # my_list = predictions.select(f.collect_list('sentence_embedding_dense')).first()[0]\n# my_list = predictions.select(\"sentence_embedding_densed\").rdd.flatMap(lambda x: x).collect()","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.713295Z","iopub.status.idle":"2023-04-09T15:41:13.713926Z","shell.execute_reply.started":"2023-04-09T15:41:13.713581Z","shell.execute_reply":"2023-04-09T15:41:13.713613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submit = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.716077Z","iopub.status.idle":"2023-04-09T15:41:13.716671Z","shell.execute_reply.started":"2023-04-09T15:41:13.716370Z","shell.execute_reply":"2023-04-09T15:41:13.716401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_predictions = predictions.toPandas()","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.720358Z","iopub.status.idle":"2023-04-09T15:41:13.721542Z","shell.execute_reply.started":"2023-04-09T15:41:13.721191Z","shell.execute_reply":"2023-04-09T15:41:13.721230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submit[\"target\"] = df_predictions.prediction","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.723211Z","iopub.status.idle":"2023-04-09T15:41:13.723703Z","shell.execute_reply.started":"2023-04-09T15:41:13.723481Z","shell.execute_reply":"2023-04-09T15:41:13.723503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submit","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.725247Z","iopub.status.idle":"2023-04-09T15:41:13.725981Z","shell.execute_reply.started":"2023-04-09T15:41:13.725759Z","shell.execute_reply":"2023-04-09T15:41:13.725786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submit.to_csv(\"submit_all_in_pyspark_logistic_regression.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.727415Z","iopub.status.idle":"2023-04-09T15:41:13.727822Z","shell.execute_reply.started":"2023-04-09T15:41:13.727617Z","shell.execute_reply":"2023-04-09T15:41:13.727637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Everything together - DL Classifier","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.729804Z","iopub.status.idle":"2023-04-09T15:41:13.730731Z","shell.execute_reply.started":"2023-04-09T15:41:13.730433Z","shell.execute_reply":"2023-04-09T15:41:13.730460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql.functions import udf, col\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml.linalg import DenseVector, VectorUDT\nfrom sparknlp.base import DocumentAssembler\nfrom sparknlp.annotator import UniversalSentenceEncoder\nfrom pyspark.ml.linalg import DenseVector, VectorUDT\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.functions import udf, explode\nfrom pyspark.ml.feature import SQLTransformer\n\n# Define the pipeline stages\nstages_one_hot = []\n\n# Define the categorical columns\ncategorical_cols = [\"keyword\", \"location\"]\n\nfor col in categorical_cols:\n    indexer = StringIndexer(inputCol=col, outputCol=col+\"_index\")\n    indexer.setHandleInvalid(\"keep\")\n    encoder = OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_vec\")\n    stages_one_hot += [indexer, encoder]\n\nstages_text = []\n# Add the DocumentAssembler stage\ndocumentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\nstages_text += [documentAssembler]\n\n# Add the UniversalSentenceEncoder stage\nencoder = UniversalSentenceEncoder.pretrained().setInputCols([\"document\"]).setOutputCol(\"sentence_embedding\")\nstages_text += [encoder]\n\nstages_together = []\nconvert_to_vector_udf = spark.udf.register(\n    \"convert_to_vector_udf\",\n    lambda r : Vectors.dense(r[0][5]), \n    VectorUDT()\n)\n\nstages_text += [SQLTransformer(\n    statement = \"SELECT *, convert_to_vector_udf(sentence_embedding) sentence_embedding_densed FROM __THIS__\")]\n\ninputCols=[col+\"_vec\" for col in categorical_cols] + [\"sentence_embedding_densed\"]\nassembler = VectorAssembler(inputCols=inputCols,\n                            outputCol=\"features\")\nstages_together += [assembler]\n\nclasssifierdl = ClassifierDLApproach()\\\n  .setInputCols([\"features\"])\\\n  .setOutputCol(\"class\")\\\n  .setLabelColumn(\"target\")\\\n  .setMaxEpochs(5)\\\n  .setEnableOutputLogs(True)\nstages_together += [classsifierdl]\n\n# Create the pipeline\nstages=stages_one_hot + stages_text + stages_together\n# stages=stages_text + stages_together\nfor stage in stages:\n    print(f\"{str(stage)} - {str(type(stage))}\")\n    if not (isinstance(stage, Estimator) or isinstance(stage, Transformer)):\n        print((\"Cannot recognize a pipeline stage of type %s.\" % type(stage)))\npipeline = Pipeline(stages=stages)\n\ntrainDataset = trainDataset.withColumn(\"target\", trainDataset[\"target\"].cast(IntegerType()))\ndf = trainDataset\n\n# Fit the pipeline to the data\npipelineModel = pipeline.fit(df)\n\n# # Apply the pipeline to the data and get the predictions\ntransformed_df = pipelineModel.transform(df)\ntransformed_df","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.732266Z","iopub.status.idle":"2023-04-09T15:41:13.733232Z","shell.execute_reply.started":"2023-04-09T15:41:13.733010Z","shell.execute_reply":"2023-04-09T15:41:13.733037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Keyword + Text","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import udf, col\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml.linalg import DenseVector, VectorUDT\nfrom sparknlp.base import DocumentAssembler\nfrom sparknlp.annotator import UniversalSentenceEncoder\nfrom pyspark.ml.linalg import DenseVector, VectorUDT\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.functions import udf, explode\nfrom pyspark.ml.feature import SQLTransformer\n\n# Define the pipeline stages\nstages_one_hot = []\n\n# Define the categorical columns\ncategorical_cols = [\"location\"]\n\nfor col in categorical_cols:\n    indexer = StringIndexer(inputCol=col, outputCol=col+\"_index\")\n    indexer.setHandleInvalid(\"keep\")\n    encoder = OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_vec\")\n    stages_one_hot += [indexer, encoder]\n\nstages_text = []\n# Add the DocumentAssembler stage\ndocumentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\nstages_text += [documentAssembler]\n\n# Add the UniversalSentenceEncoder stage\nencoder = UniversalSentenceEncoder.pretrained().setInputCols([\"document\"]).setOutputCol(\"sentence_embedding\")\nstages_text += [encoder]\n\nstages_together = []\nconvert_to_vector_udf = spark.udf.register(\n    \"convert_to_vector_udf\",\n    lambda r : Vectors.dense(r[0][5]), \n    VectorUDT()\n)\n\nstages_text += [SQLTransformer(\n#     statement = \"SELECT convert_to_vector_udf(keyword_vec) keyword_vec_densed, convert_to_vector_udf(location_vec) location_vec_densed, convert_to_vector_udf(sentence_embedding) sentence_embedding_densed, target FROM __THIS__\")]\n    statement = \"SELECT *, convert_to_vector_udf(sentence_embedding) sentence_embedding_densed FROM __THIS__\")]\n\ninputCols=[col+\"_vec\" for col in categorical_cols] + [\"sentence_embedding_densed\"]\n# inputCols=[\"sentence_embedding_densed\"]\nassembler = VectorAssembler(inputCols=inputCols,\n                            outputCol=\"features\")\nstages_together += [assembler]\n\n# Add the LogisticRegression stage with the target column as the label\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"target\")\nstages_together += [lr]\n\n# classsifierdl = ClassifierDLApproach()\\\n#   .setInputCols([\"features\"])\\\n#   .setOutputCol(\"class\")\\\n#   .setLabelColumn(\"target\")\\\n#   .setMaxEpochs(5)\\\n#   .setEnableOutputLogs(True)\n# stages_together += [classsifierdl]\n\n# Create the pipeline\nstages=stages_one_hot + stages_text + stages_together\n# stages=stages_text + stages_together\nfor stage in stages:\n    print(f\"{str(stage)} - {str(type(stage))}\")\n    if not (isinstance(stage, Estimator) or isinstance(stage, Transformer)):\n        print((\"Cannot recognize a pipeline stage of type %s.\" % type(stage)))\npipeline = Pipeline(stages=stages)\n\ntrainDataset = trainDataset.withColumn(\"target\", trainDataset[\"target\"].cast(IntegerType()))\ndf = trainDataset\n\n# Fit the pipeline to the data\npipelineModel = pipeline.fit(df)\n\n# # Apply the pipeline to the data and get the predictions\ntransformed_df = pipelineModel.transform(df)\ntransformed_df","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.735167Z","iopub.status.idle":"2023-04-09T15:41:13.735611Z","shell.execute_reply.started":"2023-04-09T15:41:13.735393Z","shell.execute_reply":"2023-04-09T15:41:13.735415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testDataset = spark.read \\\n    .option(\"header\", True) \\\n    .option('multiLine', 'true') \\\n    .csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.737241Z","iopub.status.idle":"2023-04-09T15:41:13.738300Z","shell.execute_reply.started":"2023-04-09T15:41:13.738028Z","shell.execute_reply":"2023-04-09T15:41:13.738066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = pipelineModel.transform(testDataset)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.740077Z","iopub.status.idle":"2023-04-09T15:41:13.740546Z","shell.execute_reply.started":"2023-04-09T15:41:13.740316Z","shell.execute_reply":"2023-04-09T15:41:13.740342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submit = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.742348Z","iopub.status.idle":"2023-04-09T15:41:13.742857Z","shell.execute_reply.started":"2023-04-09T15:41:13.742611Z","shell.execute_reply":"2023-04-09T15:41:13.742635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_predictions = predictions.toPandas()","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.744932Z","iopub.status.idle":"2023-04-09T15:41:13.745642Z","shell.execute_reply.started":"2023-04-09T15:41:13.745403Z","shell.execute_reply":"2023-04-09T15:41:13.745433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submit[\"target\"] = df_predictions.prediction","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.746843Z","iopub.status.idle":"2023-04-09T15:41:13.748079Z","shell.execute_reply.started":"2023-04-09T15:41:13.747824Z","shell.execute_reply":"2023-04-09T15:41:13.747849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submit.to_csv(\"submit_keyword_and_text_pyspark_logistic_regression.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-04-09T15:41:13.749670Z","iopub.status.idle":"2023-04-09T15:41:13.750125Z","shell.execute_reply.started":"2023-04-09T15:41:13.749915Z","shell.execute_reply":"2023-04-09T15:41:13.749938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}